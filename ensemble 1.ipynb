{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2659a6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec113456",
   "metadata": {},
   "outputs": [],
   "source": [
    "_id=1510\n",
    "X_breast,y_breast= fetch_openml(data_id=_id,return_X_y=True,parser='auto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c75a853d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_id=182\n",
    "X_sat,y_sat= fetch_openml(data_id=_id,return_X_y=True,parser='auto')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2a5edb",
   "metadata": {},
   "source": [
    "# Bagging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d7b4035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the number of base classifiers\n",
    "n_classifiers = 20\n",
    "k=1\n",
    "\n",
    "# create a list to store the base classifiers\n",
    "def Bagging_DT(X_train, X_test, y_train, y_test,n_classifiers):\n",
    "    classifiers = []\n",
    "    # train n_classifiers decision trees with depth 1 on different subsets of the training data\n",
    "    np.random.seed(0)\n",
    "    for i in range(n_classifiers):\n",
    "        \n",
    "        # randomly sample the training data with replacement to create a new training set\n",
    "        sample_indices = np.random.choice(len(X_train), len(X_train), replace=True)\n",
    "        X_train_sample = X_train.iloc[sample_indices]\n",
    "        y_train_sample = y_train.iloc[sample_indices]\n",
    "\n",
    "        # fit a decision tree with depth 1 on the sample training data\n",
    "        clf = DecisionTreeClassifier(max_depth=1)\n",
    "        clf.fit(X_train_sample, y_train_sample)\n",
    "\n",
    "        # store the fitted classifier in the list\n",
    "        classifiers.append(clf)\n",
    "\n",
    "    # use the base classifiers to make predictions on the test data\n",
    "    predictions = np.ones((len(X_test), n_classifiers))\n",
    "    for i, clf in enumerate(classifiers):\n",
    "        predictions[:, i] = clf.predict(X_test)\n",
    "\n",
    "    # take the majority vote to make the final prediction for each test data point\n",
    "    final_predictions = np.round(np.mean(predictions, axis=1))\n",
    "    # calculate the accuracy of the bagging algorithm\n",
    "    y_test = y_test.astype(int)\n",
    "    accuracy = np.mean(final_predictions == y_test)\n",
    "    #print('Accuracy:', accuracy)\n",
    "    return final_predictions,accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e865b042",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bagging_KNN(X_train, X_test, y_train, y_test, n_classifiers, k):\n",
    "    classifiers = []\n",
    "    # train n_classifiers KNN classifiers on different subsets of the training data\n",
    "    np.random.seed(0)\n",
    "    for i in range(n_classifiers):\n",
    "        \n",
    "        # randomly sample the training data with replacement to create a new training set\n",
    "        sample_indices = np.random.choice(len(X_train), len(X_train), replace=True)\n",
    "        X_train_sample = X_train.iloc[sample_indices]\n",
    "        y_train_sample = y_train.iloc[sample_indices]\n",
    "\n",
    "        # fit a KNN classifier on the sample training data\n",
    "        clf = KNeighborsClassifier(n_neighbors=k)\n",
    "        clf.fit(X_train_sample, y_train_sample)\n",
    "\n",
    "        # store the fitted classifier in the list\n",
    "        classifiers.append(clf)\n",
    "\n",
    "    # use the base classifiers to make predictions on the test data\n",
    "    predictions = np.ones((len(X_test), n_classifiers))\n",
    "    for i, clf in enumerate(classifiers):\n",
    "        predictions[:, i] = clf.predict(X_test)\n",
    "\n",
    "    # take the majority vote to make the final prediction for each test data point\n",
    "    final_predictions = np.round(np.mean(predictions, axis=1))\n",
    "    # calculate the accuracy of the bagging algorithm\n",
    "    y_test = y_test.astype(int)\n",
    "    accuracy = np.mean(final_predictions == y_test)\n",
    "    #print('Accuracy:', accuracy)\n",
    "    return final_predictions, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c01f143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DT_Cross_Val(X,y):\n",
    "    n_classifiers_range =(20,30,50,100,200,250)\n",
    "    accuracies = []\n",
    "\n",
    "    # initialize the stratified k-fold cross-validation object\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "    # loop over the range of classifiers\n",
    "    for n_classifiers in n_classifiers_range:\n",
    "        accuracy = []\n",
    "\n",
    "        # loop over the folds\n",
    "        for train_index, test_index in kfold.split(X, y):\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            pred,acc=Bagging_DT(X_train,X_test,y_train,y_test,n_classifiers)\n",
    "            accuracy.append(acc)\n",
    "        #print(accuracy,'\\n')\n",
    "        accuracies.append(np.mean(accuracy))   \n",
    "    optimal_n_classifiers = n_classifiers_range[np.argmax(accuracies)]\n",
    "    print('mean of accuracies: ',accuracies)\n",
    "    print('Optimal number of classifiers:', optimal_n_classifiers)\n",
    "    print('Optimal accuracy:', accuracies[np.argmax(accuracies)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b81a051b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN_Cross_Val(X,y):\n",
    "    n_classifiers_range =(20,30,50,100,200,250)\n",
    "    accuracies = []\n",
    "\n",
    "    # initialize the stratified k-fold cross-validation object\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "    # loop over the range of classifiers\n",
    "    for n_classifiers in n_classifiers_range:\n",
    "        accuracy = []\n",
    "\n",
    "        # loop over the folds\n",
    "        for train_index, test_index in kfold.split(X, y):\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            pred,acc=Bagging_KNN(X_train,X_test,y_train,y_test,n_classifiers,k=7)\n",
    "            accuracy.append(acc)\n",
    "        #print(accuracy,'\\n')\n",
    "        accuracies.append(np.mean(accuracy))   \n",
    "    optimal_n_classifiers = n_classifiers_range[np.argmax(accuracies)]\n",
    "    print('mean of accuracies: ',accuracies)\n",
    "    print('Optimal number of classifiers:', optimal_n_classifiers)\n",
    "    print('Optimal accuracy:', accuracies[np.argmax(accuracies)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0b1bb8",
   "metadata": {},
   "source": [
    "# breast dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfa60e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9385964912280702\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_breast, y_breast, test_size=0.2, random_state=0)\n",
    "breast_pred,acc=Bagging_DT(X_train, X_test, y_train, y_test,n_classifiers)\n",
    "print('Accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ed9b770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9035087719298246\n"
     ]
    }
   ],
   "source": [
    "breast_pred,acc=Bagging_KNN(X_train, X_test, y_train, y_test,n_classifiers,k)\n",
    "print('Accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c9e6d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of accuracies:  [0.9121409718987735, 0.915649743828598, 0.9156342182890856, 0.9191274646793977, 0.9261605340785592, 0.9208818506443098]\n",
      "Optimal number of classifiers: 200\n",
      "Optimal accuracy: 0.9261605340785592\n"
     ]
    }
   ],
   "source": [
    "DT_Cross_Val(X_breast,y_breast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7738d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of accuracies:  [0.9332401800962582, 0.9349945660611706, 0.9332401800962582, 0.9332401800962582, 0.9349945660611706, 0.9385033379909953]\n",
      "Optimal number of classifiers: 250\n",
      "Optimal accuracy: 0.9385033379909953\n"
     ]
    }
   ],
   "source": [
    "KNN_Cross_Val(X_breast,y_breast)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc95fdd",
   "metadata": {},
   "source": [
    "# satimage dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a015e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.307153965785381\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_sat, y_sat, test_size=0.2, random_state=0)\n",
    "y_test=y_test.astype(float)\n",
    "y_test=y_test.astype(int)\n",
    "y_train=y_train.astype(float)\n",
    "y_train=y_train.astype(int)\n",
    "sat_pred,acc=Bagging_DT(X_train, X_test, y_train, y_test,n_classifiers)\n",
    "print('Accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95bff631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8600311041990669\n"
     ]
    }
   ],
   "source": [
    "breast_pred,acc=Bagging_KNN(X_train, X_test, y_train, y_test,n_classifiers,k)\n",
    "print('Accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4415b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of accuracies:  [0.35303265940902023, 0.35303265940902023, 0.3052877138413686, 0.302954898911353, 0.30451010886469676, 0.30451010886469676]\n",
      "Optimal number of classifiers: 20\n",
      "Optimal accuracy: 0.35303265940902023\n"
     ]
    }
   ],
   "source": [
    "y_sat=y_sat.astype(float)\n",
    "DT_Cross_Val(X_sat,y_sat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c8e0888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of accuracies:  [0.8598755832037325, 0.8618973561430792, 0.864230171073095, 0.862208398133748, 0.8626749611197513, 0.8626749611197513]\n",
      "Optimal number of classifiers: 50\n",
      "Optimal accuracy: 0.864230171073095\n"
     ]
    }
   ],
   "source": [
    "KNN_Cross_Val(X_sat,y_sat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dffa889",
   "metadata": {},
   "source": [
    "# OOB Breast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ff899fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bagging_DT_oob(X_train, X_test, y_train, y_test, n_classifiers):\n",
    "    classifiers = []\n",
    "    # train n_classifiers decision trees with depth 1 on different subsets of the training data\n",
    "    np.random.seed(0)\n",
    "    n_samples = len(X_train)\n",
    "    oob_accuracies=[]\n",
    "    y_train=y_train.astype(float)\n",
    "    for i in range(n_classifiers):\n",
    "        \n",
    "        # randomly sample the training data with replacement to create a new training set\n",
    "        sample_indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "        X_train_sample = X_train.iloc[sample_indices]\n",
    "        y_train_sample = y_train.iloc[sample_indices]\n",
    "\n",
    "        # fit a decision tree with depth 1 on the sample training data\n",
    "        clf = DecisionTreeClassifier(max_depth=1)\n",
    "        clf.fit(X_train_sample, y_train_sample)\n",
    "\n",
    "        # store the fitted classifier in the list\n",
    "        classifiers.append(clf)\n",
    "\n",
    "        # use the current classifier to make predictions on the samples not included in the current training set\n",
    "        not_included_indices = np.array([i for i in range(n_samples) if i not in sample_indices])\n",
    "        not_inc_pred= clf.predict(X_train.iloc[not_included_indices]).astype(int)\n",
    "        oob_accuracies.append(np.mean(not_inc_pred==y_train_sample.iloc[not_included_indices]))\n",
    "\n",
    "\n",
    "    # average the OOB predictions over all classifiers to get the final OOB prediction\n",
    "\n",
    "    oob_error = 1- np.mean(oob_accuracies)\n",
    "    #oob_accuracy = np.mean(oob_predictions == y_train)\n",
    "    #print('OOB accuracy:', oob_accuracy)\n",
    "    \n",
    "    # use the base classifiers to make predictions on the test data\n",
    "    predictions = np.ones((len(X_test), n_classifiers))\n",
    "    for i, clf in enumerate(classifiers):\n",
    "        predictions[:, i] = clf.predict(X_test)\n",
    "\n",
    "    # take the majority vote to make the final prediction for each test data point\n",
    "    final_predictions = np.round(np.mean(predictions, axis=1))\n",
    "    # calculate the accuracy of the bagging algorithm\n",
    "    y_test = y_test.astype(int)\n",
    "    accuracy = np.mean(final_predictions == y_test)\n",
    "    #print('Accuracy:', accuracy)\n",
    "    #print(oob_accuracies)\n",
    "    return final_predictions, accuracy, oob_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a35a6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bagging_KNN_OOB(X_train, X_test, y_train, y_test, n_classifiers, k):\n",
    "    classifiers = []\n",
    "    n_samples = len(X_train)\n",
    "    oob_accuracies=[]\n",
    "    y_train=y_train.astype(float)\n",
    "    np.random.seed(0)\n",
    "    for i in range(n_classifiers):\n",
    "        \n",
    "        # randomly sample the training data with replacement to create a new training set\n",
    "        sample_indices = np.random.choice(len(X_train), len(X_train), replace=True)\n",
    "        X_train_sample = X_train.iloc[sample_indices]\n",
    "        y_train_sample = y_train.iloc[sample_indices]\n",
    "\n",
    "        # fit a KNN classifier on the sample training data\n",
    "        clf = KNeighborsClassifier(n_neighbors=k)\n",
    "        clf.fit(X_train_sample, y_train_sample)\n",
    "\n",
    "        # store the fitted classifier in the list\n",
    "        classifiers.append(clf)\n",
    "        not_included_indices = np.array([i for i in range(n_samples) if i not in sample_indices])\n",
    "        not_inc_pred= clf.predict(X_train.iloc[not_included_indices]).astype(int)\n",
    "        oob_accuracies.append(np.mean(not_inc_pred==y_train_sample.iloc[not_included_indices]))\n",
    "\n",
    "    # use the base classifiers to make predictions on the test data\n",
    "    predictions = np.ones((len(X_test), n_classifiers))\n",
    "    for i, clf in enumerate(classifiers):\n",
    "        predictions[:, i] = clf.predict(X_test)\n",
    "\n",
    "    # take the majority vote to make the final prediction for each test data point\n",
    "    final_predictions = np.round(np.mean(predictions, axis=1))\n",
    "    # calculate the accuracy of the bagging algorithm\n",
    "    y_test = y_test.astype(int)\n",
    "    accuracy = np.mean(final_predictions == y_test)\n",
    "\n",
    "    # use the samples that are not included in the training of each base classifier to make OOB predictions\n",
    "    oob_error = 1- np.mean(oob_accuracies)\n",
    "    #print('OOB accuracy:', oob_accuracy)\n",
    "   \n",
    "    #print('Accuracy:', accuracy)\n",
    "    #print('OOB Accuracy:', oob_accuracy)\n",
    "    return final_predictions, accuracy, oob_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0861410c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.956140350877193\n",
      "oob error: 0.46153887606313115\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_breast, y_breast, test_size=0.2, random_state=0)\n",
    "breast_pred,acc,oob=Bagging_DT_oob(X_train, X_test, y_train, y_test,200)\n",
    "print('Accuracy:', acc)\n",
    "print('oob error:', oob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f04f5873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9122807017543859\n",
      "oob error: 0.4584218169909724\n"
     ]
    }
   ],
   "source": [
    "breast_pred,acc,oob=Bagging_KNN_OOB(X_train, X_test, y_train, y_test,250,k)\n",
    "print('Accuracy:', acc)\n",
    "print('oob error:', oob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c0cefd",
   "metadata": {},
   "source": [
    "# OOB satimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "09540b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.307153965785381\n",
      "oob error : 0.7630252061693837\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_sat, y_sat, test_size=0.2, random_state=0)\n",
    "breast_pred,acc,oob=Bagging_DT_oob(X_train, X_test, y_train, y_test,20)\n",
    "print('Accuracy:', acc)\n",
    "print('oob error :', oob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3e4a915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8600311041990669\n",
      "oob error: 0.8123842358062976\n"
     ]
    }
   ],
   "source": [
    "breast_pred,acc,oob=Bagging_KNN_OOB(X_train, X_test, y_train, y_test,50,k)\n",
    "print('Accuracy:', acc)\n",
    "print('oob error:', oob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ea6668",
   "metadata": {},
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e49a1552",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AdaBoost(X_train, X_test, y_train, y_test, T,base):\n",
    "    n_train = X_train.shape[0]\n",
    "    n_test = X_test.shape[0]\n",
    "    w = np.ones(n_train) / n_train\n",
    "    classifiers = []\n",
    "    alphas = []\n",
    "    \n",
    "    for t in range(T):\n",
    "        # Fit a decision stump on the training data weighted by w\n",
    "        if (base=='DT'):\n",
    "            clf = DecisionTreeClassifier(max_depth=1)\n",
    "            clf.fit(X_train, y_train, sample_weight=w)\n",
    "            classifiers.append(clf)\n",
    "        else:\n",
    "            clf = BaggingClassifier(DecisionTreeClassifier(max_depth=1),n_estimators=5)\n",
    "            clf.fit(X_train, y_train, sample_weight=w)\n",
    "            classifiers.append(clf)\n",
    "\n",
    "        # Predict on the training data to calculate weighted error\n",
    "        y_pred_train = clf.predict(X_train)\n",
    "        weighted_error = np.sum(w[y_pred_train != y_train])\n",
    "\n",
    "        # Calculate alpha\n",
    "        epsilon = 1e-10  # small constant\n",
    "        alpha = 0.5 * np.log((1 - weighted_error + epsilon) / (weighted_error + epsilon))\n",
    "\n",
    "        alphas.append(alpha)\n",
    "\n",
    "        # Update the weights for the next iteration\n",
    "        w *= np.exp(-alpha * y_train * y_pred_train)\n",
    "        w /= np.sum(w)\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    predictions = np.zeros(n_test)\n",
    "    for t, clf in enumerate(classifiers):\n",
    "        y_pred_test = clf.predict(X_test)\n",
    "        predictions += alphas[t] * y_pred_test\n",
    "\n",
    "    final_predictions = np.sign(predictions)\n",
    "    accuracy = np.mean(final_predictions == y_test)\n",
    "    return final_predictions, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f01d5f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on WDBC dataset for adaboost for 1 iterations with decision Tree: 0.9035087719298246\n",
      "Accuracy on WDBC dataset for adaboost for 10 iterations with decision Tree: 0.9649122807017544\n",
      "Accuracy on WDBC dataset for adaboost for 20 iterations with decision Tree: 0.9649122807017544\n",
      "Accuracy on WDBC dataset for adaboost for 30 iterations with decision Tree: 0.9649122807017544\n",
      "Accuracy on WDBC dataset for adaboost for 50 iterations with decision Tree: 0.9649122807017544\n",
      "Accuracy on WDBC dataset for adaboost for 70 iterations with decision Tree: 0.9649122807017544\n",
      "Accuracy on WDBC dataset for adaboost for 100 iterations with decision Tree: 0.9649122807017544\n"
     ]
    }
   ],
   "source": [
    "y_breast=y_breast.astype(int)\n",
    "y_breast1=y_breast-1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_breast, y_breast1, test_size=0.2, random_state=0)\n",
    "\n",
    "iterations=[1,10,20,30,50,70,100]\n",
    "for i in iterations:\n",
    "    final,accuracy=AdaBoost(X_train, X_test, y_train, y_test, i,'DT')\n",
    "    print(\"Accuracy on WDBC dataset for adaboost for\",i,\"iterations with decision Tree:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "83f24980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on WDBC dataset for adaboost for 1 iterations with bag of 5 decision trees: 0.956140350877193\n",
      "Accuracy on WDBC dataset for adaboost for 10 iterations with bag of 5 decision trees: 0.9385964912280702\n",
      "Accuracy on WDBC dataset for adaboost for 20 iterations with bag of 5 decision trees: 0.9385964912280702\n",
      "Accuracy on WDBC dataset for adaboost for 30 iterations with bag of 5 decision trees: 0.956140350877193\n",
      "Accuracy on WDBC dataset for adaboost for 50 iterations with bag of 5 decision trees: 0.9649122807017544\n",
      "Accuracy on WDBC dataset for adaboost for 70 iterations with bag of 5 decision trees: 0.9122807017543859\n",
      "Accuracy on WDBC dataset for adaboost for 100 iterations with bag of 5 decision trees: 0.9824561403508771\n"
     ]
    }
   ],
   "source": [
    "iterations=[1,10,20,30,50,70,100]\n",
    "for i in iterations:\n",
    "    final,accuracy=AdaBoost(X_train, X_test, y_train, y_test, i,'RF')\n",
    "    print(\"Accuracy on WDBC dataset for adaboost for\",i,\"iterations with bag of 5 decision trees:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2c10dd",
   "metadata": {},
   "source": [
    "Based on the results obtained, it appears that the AdaBoost algorithm performs well on the WDBC dataset, regardless of whether a single decision tree or a bag of decision trees is used as the base classifier.\n",
    "\n",
    "For the case of a single decision tree, the accuracy of the AdaBoost algorithm appears to stabilize at around 0.965 after approximately 10 iterations, and remains relatively consistent with further iterations. This suggests that adding more iterations beyond 10 does not necessarily improve the performance of the algorithm significantly.\n",
    "\n",
    "On the other hand, for the case of a bag of 5 decision trees, the accuracy of the AdaBoost algorithm appears to vary more with the number of iterations. The highest accuracy of 0.982 was achieved with 100 iterations, while the lowest accuracy of 0.912 was achieved with 70 iterations. This suggests that with a bag of decision trees, increasing the number of iterations can have a larger impact on the performance of the algorithm.\n",
    "\n",
    "Overall, it can be concluded that the AdaBoost algorithm is a powerful technique for classification, and that it can achieve high accuracy on the WDBC dataset with both single decision trees and bags of decision trees as base classifiers. However, the number of iterations used in the algorithm can have a significant impact on its performance, especially when using a bag of decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8950f92",
   "metadata": {},
   "source": [
    "# RSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "118fbe53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RSM(X_train, X_test, y_train, y_test, n_classifiers, subspace_size):\n",
    "    classifiers = []\n",
    "    feature_indices = []\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    # Select subspace_size random features for each classifier\n",
    "    for i in range(n_classifiers):\n",
    "        indices = np.random.choice(X_train.shape[1], subspace_size, replace=False)\n",
    "        feature_indices.append(indices)\n",
    "        X_train_subspace = X_train.iloc[:, indices]\n",
    "        y_train_subspace = y_train\n",
    "\n",
    "        # fit a decision tree classifier on the subspace of training data\n",
    "        clf = DecisionTreeClassifier(max_depth=1)\n",
    "        clf.fit(X_train_subspace, y_train_subspace)\n",
    "\n",
    "        # store the fitted classifier in the list\n",
    "        classifiers.append(clf)\n",
    "\n",
    "    # use the base classifiers to make predictions on the test data\n",
    "    predictions = np.ones((len(X_test), n_classifiers))\n",
    "    for i, clf in enumerate(classifiers):\n",
    "        indices = feature_indices[i]\n",
    "        X_test_subspace = X_test.iloc[:, indices]\n",
    "        predictions[:, i] = clf.predict(X_test_subspace)\n",
    "\n",
    "    # take the majority vote to make the final prediction for each test data point\n",
    "    final_predictions = np.round(np.mean(predictions, axis=1))\n",
    "    \n",
    "    # calculate the accuracy of the RSM algorithm\n",
    "    y_test = y_test.astype(int)\n",
    "    accuracy = np.mean(final_predictions == y_test)\n",
    "    \n",
    "    return final_predictions, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b767d587",
   "metadata": {},
   "source": [
    "# wdbc dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b083551f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of rsm on  the wdbc dataset :  0.9385964912280702\n"
     ]
    }
   ],
   "source": [
    "y_breast1=y_breast-1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_breast, y_breast1, test_size=0.2, random_state=0)\n",
    "n_classifiers=50\n",
    "subspace_size=5\n",
    "final,accuracy=RSM(X_train,X_test, y_train, y_test, n_classifiers, subspace_size)\n",
    "print('accuracy of rsm on  the wdbc dataset : ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60472ee5",
   "metadata": {},
   "source": [
    "# satimage dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2e395923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of rsm on  the wdbc dataset :  0.4059097978227061\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_sat, y_sat, test_size=0.2, random_state=0)\n",
    "n_classifiers=50\n",
    "subspace_size=5\n",
    "final,accuracy=RSM(X_train,X_test, y_train, y_test, n_classifiers, subspace_size)\n",
    "print('accuracy of rsm on  the wdbc dataset : ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7505660e",
   "metadata": {},
   "source": [
    "# 5CV for both datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8c4e9c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on wdbc for subspace size 5: 0.915680794907623\n",
      "Accuracy on wdbc for subspace size 10: 0.9209750038813848\n",
      "Accuracy on wdbc for subspace size 15: 0.9086166744294365\n",
      "Accuracy on wdbc for subspace size 20: 0.9015991305697874\n",
      "Accuracy on wdbc for subspace size 25: 0.8998447446048751\n",
      "Accuracy on wdbc for subspace size 30: 0.9015991305697874\n"
     ]
    }
   ],
   "source": [
    "def RSM_CV(X, y, n_splits, n_classifiers, subspace_size):\n",
    "    scores = []\n",
    "    skf = StratifiedKFold(n_splits=n_splits)\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        _, score = RSM(X_train, X_test, y_train, y_test, n_classifiers, subspace_size)\n",
    "        scores.append(score)\n",
    "    return np.mean(scores)\n",
    "n_splits = 5\n",
    "n_classifiers = 20\n",
    "subspace_sizes = [5, 10, 15, 20, 25,30]\n",
    "results = []\n",
    "for subspace_size in subspace_sizes:\n",
    "    score = RSM_CV(X_breast, y_breast1, n_splits, n_classifiers, subspace_size)\n",
    "    results.append(score)\n",
    "    \n",
    "# print the results\n",
    "for i, subspace_size in enumerate(subspace_sizes):\n",
    "    print(f\"Accuracy on wdbc for subspace size {subspace_size}: {results[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "890a0f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on satimage for subspace size 5: 0.37558320373250387\n",
      "Accuracy on satimage for subspace size 10: 0.4040435458786936\n",
      "Accuracy on satimage for subspace size 15: 0.41772939346811827\n",
      "Accuracy on satimage for subspace size 20: 0.4209953343701399\n",
      "Accuracy on satimage for subspace size 25: 0.41555209953343697\n",
      "Accuracy on satimage for subspace size 30: 0.4270606531881803\n"
     ]
    }
   ],
   "source": [
    "n_splits = 5\n",
    "n_classifiers = 20\n",
    "subspace_sizes = [5, 10, 15, 20, 25,30]\n",
    "results = []\n",
    "for subspace_size in subspace_sizes:\n",
    "    score = RSM_CV(X_sat, y_sat, n_splits, n_classifiers, subspace_size)\n",
    "    results.append(score)\n",
    "    \n",
    "# print the results\n",
    "for i, subspace_size in enumerate(subspace_sizes):\n",
    "    print(f\"Accuracy on satimage for subspace size {subspace_size}: {results[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fd43d4",
   "metadata": {},
   "source": [
    "# ECOC FOR satimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "c0b426f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def ecoc(num_classes, X_train, y_train, X_test):\n",
    "    # Create binary classifiers for all possible pairs of classes\n",
    "    binary_classifiers = []\n",
    "    for pair in itertools.combinations(range(num_classes), 2):\n",
    "        # Filter examples that belong to the current pair of classes\n",
    "        X_pair, y_pair = filtering(X_train, y_train, pair)\n",
    "        # Check if there are any examples in this pair\n",
    "        if X_pair.shape[0] > 0:\n",
    "            # Train a binary classifier on the current pair\n",
    "            clf = DecisionTreeClassifier(max_depth=1)\n",
    "            clf.fit(X_pair, y_pair)\n",
    "            binary_classifiers.append((pair, clf))\n",
    "\n",
    "    # Make predictions using the binary classifiers\n",
    "    binary_predictions = []\n",
    "    for pair, clf in binary_classifiers:\n",
    "        y_pred = clf.predict(X_test)\n",
    "        binary_predictions.append((pair, y_pred))\n",
    "\n",
    "    # Combine the binary predictions to get multi-class predictions\n",
    "    multi_class_predictions = []\n",
    "    for i in range(len(X_test)):\n",
    "        # Count the number of votes for each class\n",
    "        votes = [0] * num_classes\n",
    "        for pair, y_pred in binary_predictions:\n",
    "            if y_pred[i] == 0:\n",
    "                # Add a vote for the first class in the current pair\n",
    "                votes[pair[0]] += 1\n",
    "            else:\n",
    "                # Add a vote for the second class in the current pair\n",
    "                votes[pair[1]] += 1\n",
    "        # Choose the class with the most votes\n",
    "        multi_class_predictions.append(votes.index(max(votes)))\n",
    "\n",
    "    return multi_class_predictions\n",
    "\n",
    "def filtering(X, y, pair):\n",
    "    # Filter examples that belong to the current pair of classes\n",
    "    mask = (y == pair[0]) | (y == pair[1])\n",
    "    X_filtered = X[mask]\n",
    "    y_filtered = y[mask]\n",
    "    # Convert class labels to binary labels\n",
    "    y_filtered[y_filtered == pair[0]] = 0\n",
    "    y_filtered[y_filtered == pair[1]] = 1\n",
    "    return X_filtered, y_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "19a5c6b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8133748055987559\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sat, y_sat, test_size=0.2, random_state=42)\n",
    "y_pred = ecoc(num_classes=10, X_train=X_train, y_train=y_train, X_test=X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f609c1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
